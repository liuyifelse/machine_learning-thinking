决策树：

利用熵来作为分类原则进行样本的划分。
三种不同的划分方式：
  （1）ID3 ：信息增益：g(D,A) = H(D) - H(D|A) 信息增益越大，换句话说，就是使得信息熵下降最快的属性越好
  （2）C4.5:信息增益率     由于信息增益使得系统本身容易对某一个内容分类特别多的属性的偏好。
  （3）CART ：基尼系数   基尼系数越小，表示越纯，分类效果越好。
  注：基尼系数其实就是信息熵在X点处的一阶泰勒展开

还么有学会怎么在这个里面打公式，暂时先放着不管。

DT算法建树的过程，其实就是根据信息增益（ID3），选择属性进行分类，然后去掉刚才已经使用完的属性，不断递归的建下一层。

这种过程带来很大的问题，就是非常容易过拟合。
  过拟合：学术定义就是，算法在训练集上表现的特别好，但是在测试集上表现的很差。其实质就是，算法为了追求准确率过分的将样本本身的特点当作总体的一般性特性进行
学习的过程。

解决决策树容易过拟合的方法：
    预剪枝：（自顶向下）其过程如下：每一次计算完最有属性之后，并不是立刻进行属性的划分和建树，而是计算一遍，如果不进行属性的划分和进行属性的划分，对于系统的准确率有没有
 上升，如果划分属性并没有带来准确率的上升，那么这次划分属性是没有意义的，并且徒增了计算量、计算参数，增添了过拟合的风险。
   后剪枝：（自底向上）其过程如下： 在一颗完整的树建立之后，自底向上的检查每一课分支是否能够带来系统准确率的上升，如果不能，那么将被剪掉。后剪枝的空间消耗和计算
   资源相对于预剪枝来说，都要大很多。
 
 优缺点：
    决策树对于异常值不敏感，CART还能够处理连续值的问题，利用信息熵作为分类准则，数学支撑不错。
    然后就是几种不同的分类准则各自的优缺点，比如ID3对于某些属性自身类别太多的偏好性。C4.5不能够处理连续值，并且对于某些属性类别特少的偏好性。相对来说，
    CART还是具备着不错的性质，而且SK-learn中实现了调优的CART。
 缺点：很容易过拟合，还有一些需要不断的补充。
 
 
 决策树跟其他算法至今的联系;
      DT -----随机森林（后面继续总结）
      还有集成学习的其他算法，比如GBDT，XGboost等。这几个算法好用，简单，容易出效果，在很多的kaggle比赛上都能看到他们的身影，后面专题再聊。
      
      
      
学习决策树的时候，产生的疑惑：
  决策树本身属于判别式模型，其代价函数是什么？正则化是什么？
      预答：决策树的训练过程是根据信息熵下降最快的原则来进行分类的。
      
